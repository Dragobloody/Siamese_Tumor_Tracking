{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy, multiprocessing\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from model import get_sr_G, get_dx_G, get_D, SRGAN_d2, get_patch_D, cycle_G\n",
    "from config import config\n",
    "\n",
    "\n",
    "import os\n",
    "import dicom_to_numpy as dtn\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gans import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Set training hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###====================== HYPER-PARAMETERS ===========================###\n",
    "## Adam\n",
    "batch_size = config.TRAIN.batch_size\n",
    "batch_size = 4\n",
    "lr_init = config.TRAIN.lr_init\n",
    "beta1 = config.TRAIN.beta1\n",
    "## initialize G\n",
    "n_epoch_init = config.TRAIN.n_epoch_init\n",
    "## adversarial learning (SRGAN)\n",
    "n_epoch = config.TRAIN.n_epoch\n",
    "lr_decay = config.TRAIN.lr_decay\n",
    "decay_every = config.TRAIN.decay_every\n",
    "shuffle_buffer_size = 128\n",
    "\n",
    "\n",
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "checkpoint_dir = \"models\"\n",
    "tl.files.exists_or_mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load and preprocess training DRRs and X-Rays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = ''\n",
    "drr_imgs = dtn.load_data(load_dir + 'drrs.hdf5', 'drrs')\n",
    "xray_imgs = dtn.load_data(load_dir + 'xrays.hdf5', 'xrays')\n",
    "\n",
    "\n",
    "drr_imgs = drr_imgs[:360, 128:, 128:-128]\n",
    "xray_imgs = xray_imgs[:360, 128:, 128:-128]\n",
    "\n",
    "\n",
    "c = 1/np.log(1 + np.max(drr_imgs, axis = (1,2)))\n",
    "drr_imgs = np.log(drr_imgs+1)\n",
    "drr_imgs = np.multiply(c[..., np.newaxis, np.newaxis], drr_imgs)\n",
    "drr_imgs = (drr_imgs - np.min(drr_imgs))/(np.max(drr_imgs)-np.min(drr_imgs))\n",
    "drr_imgs = drr_imgs*2 - 1\n",
    "\n",
    "c = 1/np.log(1 + np.max(xray_imgs, axis = (1,2)))\n",
    "xray_imgs = np.log(xray_imgs+1)\n",
    "xray_imgs = np.multiply(c[..., np.newaxis, np.newaxis], xray_imgs)\n",
    "xray_imgs = (xray_imgs - np.min(xray_imgs))/(np.max(xray_imgs)-np.min(xray_imgs))\n",
    "xray_imgs = xray_imgs*2 - 1\n",
    "xray_imgs = gamma_transform(xray_imgs, 5)\n",
    "\n",
    "\n",
    "\n",
    "drr_imgs = drr_imgs[..., np.newaxis]\n",
    "xray_imgs = xray_imgs[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train SRGAN Generator alone for initialization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------------------INITIALIZE G_SR------------------------------------\n",
    "           \n",
    "    ## initialize learning G_sr\n",
    "    G_sr.train() \n",
    "    \n",
    "    n_step_epoch = round(n_epoch_init // batch_size)\n",
    "    for epoch in range(n_epoch_init):       \n",
    "        for step, (drr_lr_patchs, drr_hr_patchs, _, _) in enumerate(train_sr):           \n",
    "            if drr_lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                fake_drr_hr_patchs = G_sr(drr_lr_patchs)\n",
    "                #fake_xray_hr_patchs = G_sr(xray_lr_patchs)\n",
    "                drr_mse_loss_sr = tl.cost.absolute_difference_error(fake_drr_hr_patchs, drr_hr_patchs, is_mean=True)\n",
    "                #xray_mse_loss_sr = tl.cost.absolute_difference_error(fake_xray_hr_patchs, xray_hr_patchs, is_mean=True)\n",
    "                mse_loss_sr = drr_mse_loss_sr #+ xray_mse_loss_sr\n",
    "\n",
    "            grad = tape.gradient(mse_loss_sr, G_sr.trainable_weights)\n",
    "            g_sr_optimizer_init.apply_gradients(zip(grad, G_sr.trainable_weights))\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse_sr: {:.5f}\".format(\n",
    "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss_sr))\n",
    "        if (epoch != 0) and (epoch % 10 == 0):\n",
    "            tl.vis.save_images(fake_drr_hr_patchs.numpy(), [1, 2], os.path.join(save_dir, 'train_g_sr_init_{}.png'.format(epoch)))\n",
    "            G_sr.save_weights(os.path.join(checkpoint_dir, 'g_sr_init.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Train SRGAN (G + D).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------ G_SR_____D_SR------------------------------------\n",
    "    \n",
    "    ## adversarial learning (G_sr, D_sr)\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'd_sr.h5')):\n",
    "        D_sr.load_weights(os.path.join(checkpoint_dir, 'd_sr.h5'))\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'g_sr.h5')):\n",
    "        G_sr.load_weights(os.path.join(checkpoint_dir, 'g_sr.h5'))    \n",
    "   \n",
    "    D_sr.train()  \n",
    "    G_sr.train()\n",
    "\n",
    "    n_step_epoch = round(n_epoch // batch_size)\n",
    "    for epoch in range(n_epoch):\n",
    "        for step, (drr_lr_patchs, drr_hr_patchs, _, _) in enumerate(train_sr):\n",
    "            if drr_lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # generated/fake data\n",
    "                fake_drr_hr_patchs = G_sr(drr_lr_patchs)\n",
    "                \n",
    "                sr_drr_logits_fake = D_sr(fake_drr_hr_patchs)\n",
    "                sr_drr_feature_fake = VGG((fake_drr_hr_patchs+1)/2.)\n",
    "                # ground-truth/real data\n",
    "                sr_drr_logits_real = D_sr(drr_hr_patchs)\n",
    "                sr_drr_feature_real = VGG((drr_hr_patchs+1)/2.)\n",
    "                \n",
    "                # D loss  \n",
    "                d_sr_drr_loss1 = mae_criterion(sr_drr_logits_real[-1], tf.ones_like(sr_drr_logits_real[-1]))\n",
    "                d_sr_drr_loss2 = mae_criterion(sr_drr_logits_fake[-1], tf.zeros_like(sr_drr_logits_fake[-1]))      \n",
    "              \n",
    "                d_loss = d_sr_drr_loss1 +  d_sr_drr_loss2\n",
    "                         #+ d_sr_xray_loss1 +  d_sr_xray_loss2)/2.                           \n",
    "               \n",
    "                                          \n",
    "                # G_sr super resolution loss\n",
    "                g_sr_drr_gan_loss = 1e-2 * mae_criterion(sr_drr_logits_fake[-1], tf.ones_like(sr_drr_logits_fake[-1]))\n",
    "                g_sr_drr_ade_loss = tl.cost.absolute_difference_error(fake_drr_hr_patchs, drr_hr_patchs, is_mean=True)\n",
    "                g_sr_drr_vgg_loss = content_loss(sr_drr_feature_fake, sr_drr_feature_real, lamda=2e-6)\n",
    "\n",
    "                g_sr_loss = g_sr_drr_gan_loss\\\n",
    "                            + g_sr_drr_ade_loss\\\n",
    "                            + g_sr_drr_vgg_loss\n",
    "                                   \n",
    "            \n",
    "            grad = tape.gradient(d_loss, D_sr.trainable_weights)\n",
    "            d_optimizer.apply_gradients(zip(grad, D_sr.trainable_weights))           \n",
    "            grad = tape.gradient(g_sr_loss, G_sr.trainable_weights)\n",
    "            g_sr_optimizer.apply_gradients(zip(grad, G_sr.trainable_weights))            \n",
    "            \n",
    "           \n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_sr_loss(ade:{:.5f}, content:{:.5f}, adv:{:.5f}) d_loss:{:.5f}\".format(\n",
    "                epoch, n_epoch, step, n_step_epoch, time.time() - step_time,\n",
    "                g_sr_drr_ade_loss, \n",
    "                g_sr_drr_vgg_loss, \n",
    "                g_sr_drr_gan_loss,\n",
    "                d_loss,\n",
    "                ))\n",
    "         \n",
    "        # update the learning rate\n",
    "        if epoch > 100 and (epoch % decay_every == 0):\n",
    "            new_lr_decay = lr_decay**(epoch // decay_every)\n",
    "            lr_v.assign(lr_init * new_lr_decay)\n",
    "            log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
    "            print(log)\n",
    "            \n",
    "        \n",
    "        result = np.concatenate([drr_hr_patchs.numpy(),\n",
    "                                 fake_drr_hr_patchs.numpy(),\n",
    "                                 tf.image.resize(drr_lr_patchs, size=[size, size], method=\"bicubic\").numpy()], axis=0)\n",
    "\n",
    "        \n",
    "        tl.vis.save_images(result, [3, 2], os.path.join(save_dir, 'train_g_sr_{}.png'.format(epoch)))\n",
    "        G_sr.save_weights(os.path.join(checkpoint_dir, 'g_sr.h5'))\n",
    "        D_sr.save_weights(os.path.join(checkpoint_dir, 'd_sr.h5'))\n",
    "            \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Train CycleGAN Generators (G1 and G2) for initialization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#---------------------------------INITIALIZE G_DX------------------------------------\n",
    "    \n",
    "    ## initialize learning G_dx    \n",
    "    G1.train() \n",
    "    G2.train()\n",
    "    \n",
    "    n_step_epoch = round(n_epoch_init)\n",
    "    for epoch in range(n_epoch_init//5):       \n",
    "        for step, (_, _, drr_mr_patchs, _, _, xray_mr_patchs) in enumerate(train_dx):           \n",
    "            if drr_mr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            with tf.GradientTape(persistent=True) as tape:             \n",
    "                # generated/fake data              \n",
    "                fake_xray_patchs = G1(drr_mr_patchs) \n",
    "                #cycled_drr_patchs = G2(fake_xray_patchs)\n",
    "                fake_drr_patchs = G2(xray_mr_patchs)  \n",
    "                #cycled_xray_patchs = G1(fake_drr_patchs) \n",
    "               \n",
    "                xray_ade_loss = tl.cost.mean_squared_error(fake_xray_patchs, xray_mr_patchs, is_mean=True)\n",
    "                drr_ade_loss = tl.cost.mean_squared_error(fake_drr_patchs, drr_mr_patchs, is_mean=True)\n",
    "                \n",
    "                g1_loss = xray_ade_loss #+ cycle_loss\n",
    "                g2_loss = drr_ade_loss #+ cycle_loss\n",
    "            \n",
    "            \n",
    "            grad = tape.gradient(g1_loss, G1.trainable_weights)\n",
    "            g1_optimizer_init.apply_gradients(zip(grad, G1.trainable_weights))\n",
    "            grad = tape.gradient(g2_loss, G2.trainable_weights)\n",
    "            g2_optimizer_init.apply_gradients(zip(grad, G2.trainable_weights))\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g1_ade: {:.5f}, g2_ade: {:.5f}\".format(\n",
    "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time,\n",
    "                xray_ade_loss,\n",
    "                drr_ade_loss,\n",
    "                ))            \n",
    "        \n",
    "        tl.vis.save_images(fake_xray_patchs.numpy(), [2, 3], os.path.join(save_dir, 'train_g1_dx_init_{}.png'.format(epoch)))\n",
    "        tl.vis.save_images(fake_drr_patchs.numpy(), [2, 3], os.path.join(save_dir, 'train_g2_dx_init_{}.png'.format(epoch)))\n",
    "        G1.save_weights(os.path.join(checkpoint_dir, 'g1_dx_init.h5'))\n",
    "        G2.save_weights(os.path.join(checkpoint_dir, 'g2_dx_init.h5'))\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train CycleGAN (G1 + D1 + G2 + D2).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------ G_DX_____D_DX------------------------------------\n",
    "               \n",
    "    ## adversarial learning (G_dx, D_dx)\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'd1_dx.h5')):\n",
    "        D1.load_weights(os.path.join(checkpoint_dir, 'd1_dx.h5'))\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'd2_dx.h5')):\n",
    "        D2.load_weights(os.path.join(checkpoint_dir, 'd2_dx.h5'))\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'g1_dx.h5')):\n",
    "        G1.load_weights(os.path.join(checkpoint_dir, 'g1_dx.h5'))\n",
    "    if tl.files.file_exists(os.path.join(checkpoint_dir, 'g2_dx.h5')):\n",
    "        G2.load_weights(os.path.join(checkpoint_dir, 'g2_dx.h5'))\n",
    "   \n",
    "    D1.train()\n",
    "    D2.train()\n",
    "    G1.train()\n",
    "    G2.train()\n",
    "    \n",
    "    \n",
    "    n_step_epoch = round(n_epoch // batch_size)\n",
    "    for epoch in range(n_epoch):  \n",
    "        for step, (_, _, drr_mr_patchs, _, _, xray_mr_patchs) in enumerate(train_dx):\n",
    "            if drr_mr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # generated/fake data              \n",
    "                fake_xray_patchs = G1(drr_mr_patchs) \n",
    "                cycled_drr_patchs = G2(fake_xray_patchs)               \n",
    "                fake_drr_patchs = G2(xray_mr_patchs) \n",
    "                cycled_xray_patchs = G1(fake_drr_patchs)               \n",
    "               \n",
    "                vgg_fake_xray_logits = D1(fake_xray_patchs)\n",
    "                vgg_fake_drr_logits = D2(fake_drr_patchs)   \n",
    "                fake_xray_features = VGG((fake_xray_patchs+1)/2.)\n",
    "                fake_drr_features = VGG((fake_drr_patchs+1)/2.)\n",
    "                \n",
    "                vgg_real_xray_logits = D1(xray_mr_patchs)\n",
    "                vgg_real_drr_logits = D2(drr_mr_patchs)\n",
    "                real_xray_features = VGG((xray_mr_patchs+1)/2.)\n",
    "                real_drr_features = VGG((drr_mr_patchs+1)/2.)\n",
    "                \n",
    "                # cycle loss\n",
    "                cycle_drr_loss = 10 * tl.cost.absolute_difference_error(cycled_drr_patchs, drr_mr_patchs, is_mean=True)\n",
    "                cycle_xray_loss = 10 * tl.cost.absolute_difference_error(cycled_xray_patchs, xray_mr_patchs, is_mean=True)\n",
    "                cycle_loss = cycle_drr_loss + cycle_xray_loss\n",
    "                \n",
    "              \n",
    "                # D1 loss              \n",
    "                d1_xray_loss1 = mae_criterion(vgg_real_xray_logits[-1:], tf.ones_like(vgg_real_xray_logits[-1:]))\n",
    "                d1_xray_loss2 = mae_criterion(vgg_fake_xray_logits[-1:], tf.zeros_like(vgg_fake_xray_logits[-1:]))\n",
    "                d1_loss = (d1_xray_loss1 + d1_xray_loss2)/2.\n",
    "                         \n",
    "                # D2 loss\n",
    "                d2_drr_loss1 = mae_criterion(vgg_real_drr_logits[-1:], tf.ones_like(vgg_real_drr_logits[-1:]))\n",
    "                d2_drr_loss2 = mae_criterion(vgg_fake_drr_logits[-1:], tf.zeros_like(vgg_fake_drr_logits[-1:]))\n",
    "                d2_loss = (d2_drr_loss1 + d2_drr_loss2)/2.\n",
    "                   \n",
    "                \n",
    "                # G1 loss\n",
    "                g1_gan_loss = mae_criterion(vgg_fake_xray_logits[-1:], tf.ones_like(vgg_fake_xray_logits[-1:]))\n",
    "                g1_xray_ade_loss = tl.cost.absolute_difference_error(fake_xray_patchs, xray_mr_patchs, is_mean=True)\n",
    "                g1_content_vgg_loss = content_loss(fake_xray_features, real_xray_features, lamda=1e-6)\n",
    "                g1_style_vgg_loss = style_loss(fake_xray_features, real_xray_features, lamda=1e-10)\n",
    "                g1_perceptual_vgg_loss = perceptual_loss(vgg_fake_xray_logits[:-1], vgg_real_xray_logits[:-1], lamda=1e-1)\n",
    "                g1_loss = g1_gan_loss\\\n",
    "                          + cycle_loss\\\n",
    "                          + g1_xray_ade_loss\\\n",
    "                          + g1_perceptual_vgg_loss\\\n",
    "                          + g1_style_vgg_loss\\\n",
    "                          + g1_content_vgg_loss\\\n",
    "                          \n",
    "                # G2 loss\n",
    "                g2_gan_loss = mae_criterion(vgg_fake_drr_logits[-1:], tf.ones_like(vgg_fake_drr_logits[-1:]))\n",
    "                g2_drr_ade_loss = tl.cost.absolute_difference_error(fake_drr_patchs, drr_mr_patchs, is_mean=True)\n",
    "                g2_content_vgg_loss = content_loss(fake_drr_features, real_drr_features, lamda=1e-6)\n",
    "                g2_style_vgg_loss = style_loss(fake_drr_features, real_drr_features, lamda=1e-10)\n",
    "                g2_perceptual_vgg_loss = perceptual_loss(vgg_fake_drr_logits[:-1], vgg_real_drr_logits[:-1], lamda=1e-1)\n",
    "                g2_loss = g2_gan_loss\\\n",
    "                          + cycle_loss\\\n",
    "                          + g2_drr_ade_loss\\\n",
    "                          + g2_perceptual_vgg_loss\\\n",
    "                          + g2_style_vgg_loss\\\n",
    "                          + g2_content_vgg_loss\\\n",
    "                \n",
    "                        \n",
    "           \n",
    "            grad = tape.gradient(g1_loss, G1.trainable_weights)\n",
    "            g1_optimizer.apply_gradients(zip(grad, G1.trainable_weights))\n",
    "            grad = tape.gradient(g2_loss, G2.trainable_weights)\n",
    "            g2_optimizer.apply_gradients(zip(grad, G2.trainable_weights))            \n",
    "            if step%1 == 0:\n",
    "                grad = tape.gradient(d1_loss, D1.trainable_weights)\n",
    "                d1_optimizer.apply_gradients(zip(grad, D1.trainable_weights))\n",
    "                grad = tape.gradient(d2_loss, D2.trainable_weights)\n",
    "                d2_optimizer.apply_gradients(zip(grad, D2.trainable_weights))            \n",
    "                        \n",
    "           \n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g1_loss(cycle:{:.5f}, ade:{:.5f}, content:{:.5f}, style:{:.5f}, perceptual:{:.5f}, adv:{:.5f}) d1_loss:{:.5f}\".format(\n",
    "                epoch, n_epoch, step, n_step_epoch, time.time() - step_time,               \n",
    "                 cycle_drr_loss, \n",
    "                 g1_xray_ade_loss,\n",
    "                 g1_content_vgg_loss,\n",
    "                 g1_style_vgg_loss,\n",
    "                 g1_perceptual_vgg_loss,\n",
    "                 g1_gan_loss, \n",
    "                 d1_loss))\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g2_loss(cycle:{:.5f}, ade:{:.5f}, content:{:.5f}, style:{:.5f}, perceptual:{:.5f}, adv:{:.5f}) d2_loss:{:.5f}\".format(\n",
    "                epoch, n_epoch, step, n_step_epoch, time.time() - step_time,               \n",
    "                 cycle_xray_loss, \n",
    "                 g2_drr_ade_loss,\n",
    "                 g2_content_vgg_loss,\n",
    "                 g2_style_vgg_loss,\n",
    "                 g2_perceptual_vgg_loss,\n",
    "                 g2_gan_loss,\n",
    "                 d2_loss))\n",
    "\n",
    "        # update the learning rate\n",
    "        if epoch > 100 and (epoch % (decay_every//2) == 0):\n",
    "            new_lr_decay = lr_decay**(epoch // (decay_every//2))\n",
    "            lr_v.assign(lr_init * new_lr_decay)\n",
    "            log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
    "            print(log)\n",
    "\n",
    "        if (epoch % 1 == 0):\n",
    "            tl.vis.save_images(fake_xray_patchs.numpy(), [2, 2], os.path.join(save_dir, 'train_g1_xrays_{}.png'.format(epoch)))\n",
    "            tl.vis.save_images(fake_drr_patchs.numpy(), [2, 2], os.path.join(save_dir, 'train_g2_drrs_{}.png'.format(epoch)))\n",
    "            G1.save_weights(os.path.join(checkpoint_dir, 'g1_dx.h5'))\n",
    "            D1.save_weights(os.path.join(checkpoint_dir, 'd1_dx.h5'))\n",
    "            G2.save_weights(os.path.join(checkpoint_dir, 'g2_dx.h5'))\n",
    "            D2.save_weights(os.path.join(checkpoint_dir, 'd2_dx.h5'))   \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
